# Predictive Marketing Analytics: Statistical Insights for Business Success

ğŸ“ˆ **Project Overview**  
This project applies advanced statistical learning techniques to predict customer subscription patterns for term deposits. Using the 'Marketing_data.csv' dataset, predictive models were developed to refine marketing strategies in the financial sector.

ğŸ“Š **Key Features**  

**ğŸ“‚ Dataset:**  
- **Source**: Inspired by the UCI Machine Learning Repository (bank.csv).  
- **Content**: 11,162 observations and 17 features, including demographic, financial, and campaign-specific attributes.  
- **Target**: Binary classification of the `deposit` variable ('yes' or 'no').

**âš™ï¸ Data Preprocessing:**  
- Feature engineering:
  - Created `age_group` and `month_part` features.  
  - Interaction effects between `pdays` and `duration`.  
- Converted categorical variables to factors.  
- Standardized numerical variables.  

**ğŸ” Exploratory Data Analysis (EDA):**  
- Visualized distributions and relationships.  
- Correlation and chi-square tests identified influential predictors.  

**ğŸ¯ Feature Selection:**  
- Logistic Lasso regression and Random Forest importance analysis highlighted key features:  
  - **Top Predictors**: Duration, month, age group, balance, housing, and previous campaign outcomes.

ğŸ› ï¸ **Methodology**  
1. **ğŸ§  Statistical Learning Models**:
   - ğŸŒ² Random Forest  
   - ğŸ“Š Logistic Regression  
   - ğŸŒ€ Support Vector Machines (SVM)  
   - ğŸŒ´ Decision Trees  

2. **ğŸ“‹ Evaluation Metrics**:  
   - Accuracy, Precision, Recall, F1-Score, ROC-AUC.  
   - Resampling techniques (bootstrapping and 5-fold cross-validation).  

ğŸ“‰ **Results**  
**Model Performance on Test Data**:  
| Model               | ğŸ† Accuracy | ğŸ¯ Precision | ğŸ” Recall | ğŸ“Š F1-Score | ğŸ“ˆ ROC-AUC |
|---------------------|-------------|--------------|-----------|-------------|------------|
| ğŸŒ² Random Forest    | 85.76%      | 89.88%       | 82.03%    | 85.77%      | 91.91%     |
| ğŸ“Š Logistic Regression | 82.17%   | 81.45%       | 85.40%    | 83.38%      | 90.31%     |
| ğŸŒ€ SVM              | 83.67%      | 85.39%       | 83.02%    | 84.18%      | 91.11%     |
| ğŸŒ´ Decision Tree    | 80.95%      | 81.02%       | 83.06%    | 82.03%      | 83.82%     |

**ğŸ’¡ Key Insights:**  
- ğŸŒ² Random Forest achieved the highest accuracy and F1-score, demonstrating superior predictive power.  
- ğŸ“Š Logistic Regression and ğŸŒ€ SVM offered competitive performance, balancing precision and recall.  
- ğŸŒ´ Decision Trees provided interpretable results with robust performance.  

ğŸš€ **Future Work**  
- ğŸŒ Integrate additional data sources (e.g., socioeconomic indicators).  
- ğŸ¤– Explore ensemble models and deep learning architectures.  
- âš–ï¸ Address dataset biases and enhance feature engineering.
